{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHFCyfdZXWO1"
   },
   "source": [
    "# LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcGq1dF0iWju"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xEXu-mePiK9T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hps1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: talos in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (1.0.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (0.0)\n",
      "Requirement already satisfied: astetik in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (1.11.1)\n",
      "Requirement already satisfied: statsmodels>=0.11.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (0.13.2)\n",
      "Requirement already satisfied: chances in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (0.1.9)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (2.8.0)\n",
      "Requirement already satisfied: wrangle in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (0.6.11)\n",
      "Requirement already satisfied: numpy in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (1.22.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (4.64.0)\n",
      "Requirement already satisfied: kerasplotlib in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (0.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (1.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from talos) (2.27.1)\n",
      "Requirement already satisfied: scipy>=1.3 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from statsmodels>=0.11.0->talos) (1.8.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from statsmodels>=0.11.0->talos) (0.5.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from statsmodels>=0.11.0->talos) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=21.3->statsmodels>=0.11.0->talos) (3.0.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from pandas->talos) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from pandas->talos) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from patsy>=0.5.2->statsmodels>=0.11.0->talos) (1.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python310\\lib\\site-packages (from tensorflow>=2.0.0->talos) (58.1.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (0.2.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (4.1.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (3.6.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (13.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (0.24.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (2.8.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (2.8.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (0.5.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (1.44.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (1.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (3.20.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.0.0->talos) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->talos) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (2.1.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (2.6.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from requests->talos) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from requests->talos) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from requests->talos) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from requests->talos) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->talos) (3.2.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from astetik->talos) (0.11.2)\n",
      "Requirement already satisfied: IPython in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from astetik->talos) (8.2.0)\n",
      "Requirement already satisfied: geonamescache in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from astetik->talos) (1.3.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (0.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (3.0.29)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (2.11.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (5.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (0.4.4)\n",
      "Requirement already satisfied: stack-data in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from IPython->astetik->talos) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->IPython->astetik->talos) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->astetik->talos) (0.2.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from kerasplotlib->talos) (3.5.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->kerasplotlib->talos) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->kerasplotlib->talos) (9.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->kerasplotlib->talos) (4.31.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->kerasplotlib->talos) (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from sklearn->talos) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sklearn->talos) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sklearn->talos) (3.1.0)\n",
      "Requirement already satisfied: executing in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->IPython->astetik->talos) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->IPython->astetik->talos) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\hps1\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->IPython->astetik->talos) (2.0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "from string import ascii_lowercase\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "!pip install talos\n",
    "import talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8KPqb7_vC59"
   },
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "96Z6cXAZiaAY"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('data/comments.csv')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8hAcigh3Sh3"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TYZ93LlRbwRB"
   },
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = train[labels].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDWRHguXxOF_"
   },
   "source": [
    "###Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z-4hq4jyhBw"
   },
   "source": [
    "#### Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4k-vI8DbqiP"
   },
   "source": [
    "* Removing Characters in between Text\n",
    "* Removing Repeated Characters\n",
    "* Converting data to lower-case\n",
    "* Removing Numbers from the data\n",
    "* Remove Punctuation\n",
    "* Remove Whitespaces\n",
    "* Removing spaces in between words\n",
    "* Removing \"\\n\"\n",
    "* Remove Non-english characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jglnA7LEbpoA"
   },
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
    "            '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
    "        ],\n",
    "\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' trans gender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "    ' homo sex ual':\n",
    "        [\n",
    "            'homosexual'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
    "                                                                                      'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu', 'st*u'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "    ' fucking ':\n",
    "        [\n",
    "            'f*$%-ing'\n",
    "        ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4RetJ1vgswOG"
   },
   "outputs": [],
   "source": [
    "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
    "\n",
    "  if is_lower:\n",
    "    text=text.lower()\n",
    "    \n",
    "  if remove_patterns_text:\n",
    "    for target, patterns in RE_PATTERNS.items():\n",
    "      for pat in patterns:\n",
    "        text=str(text).replace(pat, target)\n",
    "\n",
    "  if remove_repeat_text:\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
    "\n",
    "  text = str(text).replace(\"\\n\", \" \")\n",
    "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "  text = re.sub('[0-9]',\"\",text)\n",
    "  text = re.sub(\" +\", \" \", text)\n",
    "  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "  return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwyO_GcPuPE_"
   },
   "source": [
    "Cleaning Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Cak-Q0fl0qyb"
   },
   "outputs": [],
   "source": [
    "train['comment_text']=train['comment_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqRVmH1FRWpL"
   },
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vnhCAXkKUF9i"
   },
   "outputs": [],
   "source": [
    "comments_train=train['comment_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WwhTNF3pW7Hp"
   },
   "outputs": [],
   "source": [
    "comments_train=list(comments_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IuWMc1xqRVqV"
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fi-vYUcPoi-a"
   },
   "outputs": [],
   "source": [
    "def lemma(text, lemmatization=True):\n",
    "  output=\"\"\n",
    "  if lemmatization:\n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "       word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
    "       word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "       word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
    "       word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
    "       output=output + \" \" + word4\n",
    "  else:\n",
    "    output=text\n",
    "  \n",
    "  return str(output.strip()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX4DTjGJ-8Id"
   },
   "source": [
    "Lemmatizing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JRquFt30qWk0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hps1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6057ad9dba8648168a7dceb25fa06a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatized_train_data = [] \n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "for line in tqdm_notebook(comments_train, total=159571): \n",
    "    lemmatized_train_data.append(lemma(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZaP8BH2UG0N"
   },
   "source": [
    "#### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vNKjE4Vq8kpG"
   },
   "outputs": [],
   "source": [
    "stopword_list=STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_oqV08mMWmk"
   },
   "source": [
    "Adding Single and Dual to STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "go7RVS52I1US"
   },
   "outputs": [],
   "source": [
    "def iter_all_strings():\n",
    "    for size in itertools.count(1):\n",
    "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
    "            yield \"\".join(s)\n",
    "\n",
    "dual_alpha_list=[]\n",
    "for s in iter_all_strings():\n",
    "    dual_alpha_list.append(s)\n",
    "    if s == 'zz':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7_x3bQ4MIGVK"
   },
   "outputs": [],
   "source": [
    "dual_alpha_list.remove('i')\n",
    "dual_alpha_list.remove('a')\n",
    "dual_alpha_list.remove('am')\n",
    "dual_alpha_list.remove('an')\n",
    "dual_alpha_list.remove('as')\n",
    "dual_alpha_list.remove('at')\n",
    "dual_alpha_list.remove('be')\n",
    "dual_alpha_list.remove('by')\n",
    "dual_alpha_list.remove('do')\n",
    "dual_alpha_list.remove('go')\n",
    "dual_alpha_list.remove('he')\n",
    "dual_alpha_list.remove('hi')\n",
    "dual_alpha_list.remove('if')\n",
    "dual_alpha_list.remove('is')\n",
    "dual_alpha_list.remove('in')\n",
    "dual_alpha_list.remove('me')\n",
    "dual_alpha_list.remove('my')\n",
    "dual_alpha_list.remove('no')\n",
    "dual_alpha_list.remove('of')\n",
    "dual_alpha_list.remove('on')\n",
    "dual_alpha_list.remove('or')\n",
    "dual_alpha_list.remove('ok')\n",
    "dual_alpha_list.remove('so')\n",
    "dual_alpha_list.remove('to')\n",
    "dual_alpha_list.remove('up')\n",
    "dual_alpha_list.remove('us')\n",
    "dual_alpha_list.remove('we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "H11kkMtXMyct"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "for letter in dual_alpha_list:\n",
    "    stopword_list.add(letter)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ti67XuLCNnsL"
   },
   "source": [
    "Checking for other words that we may need in STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Z4lBqjcBaDVK"
   },
   "outputs": [],
   "source": [
    "def search_stopwords(data, search_stop=True):\n",
    "  output=\"\"\n",
    "  if search_stop:\n",
    "    data=data.split(\" \")\n",
    "    for word in data:\n",
    "      if not word in stopword_list:\n",
    "        output=output+\" \"+word \n",
    "  else:\n",
    "    output=data\n",
    "\n",
    "  return str(output.strip())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xn21RMeIbQti"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c8945a03484a8c8f37988b9bb12fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "potential_stopwords = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
    "    potential_stopwords.append(search_stopwords(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFSkmGjAZuR3"
   },
   "source": [
    "Combining all the sentences in the list into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zpmze-Q7BrK0"
   },
   "outputs": [],
   "source": [
    "def string_combine_a(stopword):\n",
    "  final_a=\"\"\n",
    "  for item in range(39893):\n",
    "    final_a=final_a+\" \"+stopword[item]\n",
    "  return final_a\n",
    "\n",
    "def string_combine_b(stopword):\n",
    "  final_b=\"\"\n",
    "  for item in range(39893,79785):\n",
    "    final_b=final_b+\" \"+stopword[item]\n",
    "  return final_b\n",
    "\n",
    "def string_combine_c(stopword):\n",
    "  final_c=\"\"\n",
    "  for item in range(79785,119678):\n",
    "    final_c=final_c+\" \"+stopword[item]\n",
    "  return final_c\n",
    "\n",
    "def string_combine_d(stopword):\n",
    "  final_d=\"\"\n",
    "  for item in range(119678,159571):\n",
    "    final_d=final_d+\" \"+stopword[item]\n",
    "  return final_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UqBc4rOahmAz"
   },
   "outputs": [],
   "source": [
    "total_string_potential_a=string_combine_a(potential_stopwords)\n",
    "total_string_potential_b=string_combine_b(potential_stopwords)\n",
    "total_string_potential_c=string_combine_c(potential_stopwords)\n",
    "total_string_potential_d=string_combine_d(potential_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbEm3D8txMSH"
   },
   "source": [
    "Counting the number of words in each of the 4 strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "O_hhbMS_xR6t"
   },
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bsh0BuT9xSDg"
   },
   "outputs": [],
   "source": [
    "total_string_potential_a_dict=word_count(total_string_potential_a)\n",
    "total_string_potential_b_dict=word_count(total_string_potential_b)\n",
    "total_string_potential_c_dict=word_count(total_string_potential_c)\n",
    "total_string_potential_d_dict=word_count(total_string_potential_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shmHkLSk3Xxp"
   },
   "source": [
    "Converting Dictionaries to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "pSJXHnv6y9T0"
   },
   "outputs": [],
   "source": [
    "total_string_potential_a_df = pd.DataFrame(list(total_string_potential_a_dict.items()),columns = ['Word','Count'])\n",
    "total_string_potential_b_df = pd.DataFrame(list(total_string_potential_b_dict.items()),columns = ['Word','Count'])\n",
    "total_string_potential_c_df = pd.DataFrame(list(total_string_potential_c_dict.items()),columns = ['Word','Count'])\n",
    "total_string_potential_d_df = pd.DataFrame(list(total_string_potential_d_dict.items()),columns = ['Word','Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjWO0vj2yaW7"
   },
   "source": [
    "Getting Dataframe output in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bzThNOjA2pjx"
   },
   "outputs": [],
   "source": [
    "top50_potential_stopwords_a=total_string_potential_a_df.sort_values(by=['Count'],ascending=False).head(50)\n",
    "top50_potential_stopwords_b=total_string_potential_b_df.sort_values(by=['Count'],ascending=False).head(50)\n",
    "top50_potential_stopwords_c=total_string_potential_c_df.sort_values(by=['Count'],ascending=False).head(50)\n",
    "top50_potential_stopwords_d=total_string_potential_d_df.sort_values(by=['Count'],ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1d8reN9CaZ8"
   },
   "source": [
    "Looking for common terms in all top 50 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AKUuEbHvCiYG"
   },
   "outputs": [],
   "source": [
    "common_potential_stopwords=list(reduce(set.intersection,map(set,[top50_potential_stopwords_a.Word,top50_potential_stopwords_b.Word,top50_potential_stopwords_c.Word,top50_potential_stopwords_d.Word])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX3lIQj0l36i"
   },
   "source": [
    "Retaining certain words and removing others from the above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zacbW5ASjN2r"
   },
   "outputs": [],
   "source": [
    "potential_stopwords=['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-yjjvj3LpZs"
   },
   "source": [
    "Adding above retrived words into the stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "73PbxjxbLvz9"
   },
   "outputs": [],
   "source": [
    "for word in potential_stopwords:\n",
    "    stopword_list.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "101v0iZjaNLR"
   },
   "source": [
    "Removing Stopwords from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "RRSga8PKsktA"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, remove_stop=True):\n",
    "  output = \"\"\n",
    "  if remove_stop:\n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "      if word not in stopword_list:\n",
    "        output=output + \" \" + word\n",
    "  else :\n",
    "    output=text\n",
    "\n",
    "  return str(output.strip())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Q-hItiV7skoV"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2859268c7e042c7b0cec7ca88df7fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_train_data = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
    "    processed_train_data.append(remove_stopwords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "a29qOSPA-rbS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agree graemel intention revert nazi protect report revert rule notice board post report revert nazi graemel revert nazi unacceptable site admins feel unjustly revert hour period report revert noticeboard'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_data[152458]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqXHFaSC-Bkf"
   },
   "source": [
    "Removing Stopwords from Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeScalvZDEdD"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SkK1P-CdX_0N"
   },
   "outputs": [],
   "source": [
    "max_features=100000      \n",
    "maxpadlen = 200          \n",
    "val_split = 0.3      \n",
    "embedding_dim_fasttext = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIeovLIr6aAo"
   },
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "qaA52PlK4xnV"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(processed_train_data))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(processed_train_data)\n",
    "word_index=tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfDwmzPj8TJf"
   },
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gEmacO337twa"
   },
   "outputs": [],
   "source": [
    "X_t=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "XtZD1j9BCrsG"
   },
   "outputs": [],
   "source": [
    "indices = np.arange(X_t.shape[0])\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "k51hkGqOnZLB"
   },
   "outputs": [],
   "source": [
    "X_t = X_t[indices]\n",
    "labels = y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWn-SkJCCswd"
   },
   "source": [
    "### Splitting data into Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "B5hOSGJVb7o4"
   },
   "outputs": [],
   "source": [
    "num_validation_samples = int(val_split*X_t.shape[0])\n",
    "x_train = X_t[: -num_validation_samples]\n",
    "y_train = labels[: -num_validation_samples]\n",
    "x_val = X_t[-num_validation_samples: ]\n",
    "y_val = labels[-num_validation_samples: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL7Mv1Z6D8_w"
   },
   "source": [
    "### Importing Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsvNLXZ6DlWS"
   },
   "outputs": [],
   "source": [
    "##PLEASE IMPORT THIS FROM https://fasttext.cc/docs/en/english-vectors.html in your working directory\n",
    "embeddings_index_fasttext = {}\n",
    "f = open('wiki-news-300d-1M.vec', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpkSOB6iEFCB"
   },
   "outputs": [],
   "source": [
    "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index_fasttext.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fasttext[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OoZ7fTpPWhi"
   },
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwzg_WX32OG3"
   },
   "source": [
    "#### Talos Grid Search  for LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZdik3-jGkye"
   },
   "outputs": [],
   "source": [
    "def toxic_classifier(x_train,y_train,x_val,y_val,params):\n",
    "\n",
    "  inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "\n",
    "  embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "  embedded_sequences = embedding_layer(inp)\n",
    "\n",
    "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "  x = GlobalMaxPool1D()(x)\n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  x = Dense(params['output_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\n",
    "  model = Model(inputs=inp, outputs=preds)\n",
    "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\n",
    "\n",
    "  return model_info, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6AtSibHGoTU"
   },
   "outputs": [],
   "source": [
    "p={\n",
    "    'output_count_lstm': [40,50,60],\n",
    "    'output_count_dense': [30,40,50],\n",
    "    'batch_size': [32],\n",
    "    'epochs':[2],\n",
    "    'optimizer':['adam'],\n",
    "    'activation':['relu'],\n",
    "    'last_activation': ['sigmoid'],\n",
    "    'dropout':[0.1,0.2],\n",
    "    'loss': ['binary_crossentropy']   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAnaHxh1GpFi"
   },
   "outputs": [],
   "source": [
    "scan_results = talos.Scan(x=x_train,\n",
    "               y=y_train,\n",
    "               x_val=x_val,\n",
    "               y_val=y_val,\n",
    "               model=toxic_classifier,\n",
    "               params=p,\n",
    "               experiment_name='tcc',\n",
    "               print_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXl1ZQSWtdYK"
   },
   "outputs": [],
   "source": [
    "model_id = scan_results.data['val_accuracy'].astype('float').argmax()\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yict0QyGxV7"
   },
   "outputs": [],
   "source": [
    "analyze_object = talos.Analyze(scan_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3d78iYYG0I3"
   },
   "outputs": [],
   "source": [
    "analyze_object.best_params('val_accuracy', ['accuracy', 'loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHrPeYI_Giky"
   },
   "source": [
    "#### Training Model with Best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIRoKKp9mzQz"
   },
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "9x04BXTCCdsV"
   },
   "outputs": [],
   "source": [
    "inp=Input(shape=(maxpadlen, ),dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-fPTQKHFF61"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "embedded_sequences = embedding_layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAF1Ui22FmHX"
   },
   "outputs": [],
   "source": [
    "#Select Model with Best Parameters from above Talos.scan \n",
    "x = LSTM(50, return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(60, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "preds = Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kigFyK4cHHrv"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mu:\\hsc\\text\\lstm.ipynb Cell 78'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/u%3A/hsc/text/lstm.ipynb#ch0000077?line=0'>1</a>\u001b[0m model_1 \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39minp, outputs\u001b[39m=\u001b[39mpreds)\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/hsc/text/lstm.ipynb#ch0000077?line=1'>2</a>\u001b[0m model_1\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/hsc/text/lstm.ipynb#ch0000077?line=2'>3</a>\u001b[0m                   optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/hsc/text/lstm.ipynb#ch0000077?line=3'>4</a>\u001b[0m                   metrics\u001b[39m=\u001b[39m[tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mAccuracy(),tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mPrecision(),tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mAUC(),tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mRecall()])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "model_1 = Model(inputs=inp, outputs=preds)\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.AUC(),tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "e_qJGa25HVxi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSqW0thULSae"
   },
   "outputs": [],
   "source": [
    "model_info_1=model_1.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wv1IdwZk_qG"
   },
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "eisFdjOUlBbM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model_LSTM_RNN\\assets\n"
     ]
    }
   ],
   "source": [
    "model_1.save(filepath=\"Model_LSTM_RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXx4IOYn4_XB"
   },
   "source": [
    "## Loading Saved Model and Print Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "huVktHqRMSGc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1496/1496 [==============================] - 42s 28ms/step - loss: 0.0456 - precision: 0.8256 - auc: 0.9836 - recall: 0.6473\n",
      "{'loss': 0.04555904120206833, 'precision': 0.8255670666694641, 'auc': 0.983612060546875, 'recall': 0.6473032832145691}\n",
      "F-Score: 0.725647403807186\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(filepath=\"Model_LSTM_RNN\")\n",
    "metrics =model.evaluate(x_val, y_val, return_dict=True)\n",
    "print(str(metrics))\n",
    "fscore = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])\n",
    "print('F-Score: ' + str(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1496/1496 [==============================] - 43s 28ms/step - loss: 0.0456 - precision: 0.8256 - auc: 0.9836 - recall: 0.6473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.04555904120206833,\n",
       " 'precision': 0.8255670666694641,\n",
       " 'auc': 0.983612060546875,\n",
       " 'recall': 0.6473032832145691}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.AUC(),tf.keras.metrics.Recall()])\n",
    "metrics =model.evaluate(x_val, y_val, return_dict=True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47871\n"
     ]
    }
   ],
   "source": [
    "truth = []\n",
    "for i in range(len(y_val)):\n",
    "    if 1 in y_val[i]:\n",
    "        truth.append(1)\n",
    "    else:\n",
    "        truth.append(0)\n",
    "print(len(truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_level(string):\n",
    "    new_string = [string]\n",
    "    print(new_string)\n",
    "    new_string = tokenizer.texts_to_sequences(new_string)\n",
    "    new_string = pad_sequences(new_string, maxlen=maxpadlen, padding='post')\n",
    "    print(type(new_string))\n",
    "    print(new_string)\n",
    "    prediction = model.predict(new_string)\n",
    "    \n",
    "    print(\"Toxicity levels for '{}':\".format(string))\n",
    "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
    "    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
    "    print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
    "    print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
    "    print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
    "    print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
    "    print()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indian people are smelly']\n",
      "<class 'numpy.ndarray'>\n",
      "[[  696     6 14867  9306     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "Toxicity levels for 'Indian people are smelly':\n",
      "Toxic:         65%\n",
      "Severe Toxic:  1%\n",
      "Obscene:       11%\n",
      "Threat:        2%\n",
      "Insult:        35%\n",
      "Identity Hate: 6%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('Indian people are smelly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "499 499\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"compiled_data.csv\")\n",
    "truth = data['Label'].values\n",
    "text = data['Text'].values\n",
    "preds = []\n",
    "for i in range(len(text)):\n",
    "    if(i%100 == 0 ):\n",
    "        print(i)\n",
    "    new_string = [text[i]]\n",
    "    new_string = tokenizer.texts_to_sequences(new_string)\n",
    "    new_string = pad_sequences(new_string, maxlen=maxpadlen, padding='post')\n",
    "    prediction = model.predict(new_string)\n",
    "    model.predict(new_string)\n",
    "    if(prediction[0][np.argmax(prediction[0])] > 0.05):\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)\n",
    "print(len(preds),len(truth))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6593186372745491\n",
      "0.7515923566878981\n",
      "0.4738955823293173\n",
      "0.58128078817734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score\n",
    "\n",
    "print(accuracy_score(truth,preds))\n",
    "print(precision_score(truth,preds))\n",
    "print(recall_score(truth,preds))\n",
    "print(f1_score(truth,preds))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
